ğŸ§  Lojistik Regresyon SÄ±nÄ±flandÄ±rma Modeli ğŸ“Š

ğŸ“ Proje AÃ§Ä±klamasÄ±
Bu Ã§alÄ±ÅŸma, YZM212 Laboratuvar Dersinin 2. Ã§alÄ±ÅŸmasÄ± kapsamÄ±nda gerÃ§ekleÅŸtirilmiÅŸtir. Projede ikili sÄ±nÄ±flandÄ±rma problemini Ã§Ã¶zmek iÃ§in Lojistik Regresyon algoritmasÄ± kullanÄ±lmÄ±ÅŸtÄ±r. 

ğŸ‘¤ Proje Sahibi
Ad Soyad: Halil Ä°brahim AkbaÅŸ
Ã–ÄŸrenci NumarasÄ±: 23291264

**Lojistik Regresyon Kodunun AÃ§Ä±klanmasÄ± ve Ã‡alÄ±ÅŸma Åekli:

LogisticRegression sÄ±nÄ±fÄ±mÄ±zdan Ã¶nce sigmoid aktivasyon fonksiyonumuzu tanÄ±mladÄ±k.

LogisticRegression (LR) sÄ±nÄ±fÄ± bu 6 temel fonksiyonlardan oluÅŸur : __init__ (SÄ±nÄ±fÄ± baÅŸlatmak iÃ§in), add_intercept (Ä°stenirse Ã¶n yargÄ±, bias, eklemek iÃ§in), cost_function (hatayÄ± hesaplamak iÃ§in, logaritmik),
fit (Modelin gradyanÄ±nÄ±n hesaplandÄ±ÄŸÄ± ve aÄŸÄ±rlÄ±klarÄ±n gÃ¼ncellendiÄŸi fonksiyon), predict_prob(Tahmin olasÄ±lÄ±klarÄ±nÄ± hesaplamak iÃ§in Sigmoid fonksiyonunu kullanÄ±r), 
ve en son olarak predict(Belirli bir eÅŸik deÄŸer Ã¼stÃ¼ iÃ§in 1 altÄ± iÃ§in 0 dÃ¶ndÃ¼rÃ¼r).

**fit(X,y) fonksiyonunun derinlemesine analizi: 

Fonksiyon ilk olarak bias var mÄ± yok mu onu kontrol eder.
SonrasÄ±nda baÅŸlangÄ±Ã§ aÄŸÄ±rlÄ±klarÄ±nÄ±, yani tetayÄ± atar (farklÄ± yÃ¶ntemler kullanÄ±labilir). Ondan sonra kaÃ§ kez iterasyon yapÄ±lmasÄ±
istenmiÅŸse o kadar kez bir dÃ¶ngÃ¼ iÃ§erisinde lineer kombinasyonlar hesaplanÄ±r ve bulunan sayÄ±sal deÄŸer sigmoid fonksiyonuna 
parametre olarak verilir (h). SonrasÄ±nda gradyan hesaplamasÄ± yapÄ±lÄ±r. Gradyan formulÃ¼ eÄŸitim verilerinin transpozu (X.T) ve model tahminleri ile gerÃ§ek etiketler arasÄ±ndaki farkÄ±n (h-y)
nokta Ã§arpÄ±mÄ± hesaplanÄ±r Ã¶zellik sayÄ±sÄ±na(y) bÃ¶lÃ¼nÃ¼r. Son olarak, bulunan gradyan ile hiperparametre Ã¶ÄŸrenme hÄ±zÄ± Ã§arpÄ±lÄ±r ve aÄŸÄ±rlÄ±klar matrisinden Ã§Ä±kartÄ±lÄ±r. Bu iÅŸlemleri iterasyon sayÄ±sÄ± kadar yapÄ±lÄ±r.

iterasyon =  gradient descent algoritmasÄ±nda modelin parametrelerini bir kez gÃ¼ncellemesi.
lineer kombinasyon = farklÄ± deÄŸiÅŸkenlerin aÄŸÄ±rlÄ±klandÄ±rÄ±lmÄ±ÅŸ toplamÄ±.
gradyan formÃ¼l = np.dot(X.T, (h - y)) / y.size 
aÄŸÄ±rlÄ±k gÃ¼ncelleme = self.theta -= self.lr * gradient




Peki, ben bu Ã§alÄ±ÅŸmada neler yaÅŸadÄ±m ? (YZ geliÅŸtirmeli)
ğŸ” Veri Ã–n Ä°ÅŸleme AÅŸamalarÄ±

1. Veri Analizi ğŸ•µï¸â€â™‚ï¸
- Veri setindeki Ã¶zelliklerin birbiriyle olan iliÅŸkilerini inceledim
- "Class" deÄŸiÅŸkeni ile diÄŸer Ã¶zellikler arasÄ±ndaki korelasyonlarÄ± kontrol ettim
- 0'a yakÄ±n doÄŸrusal iliÅŸkiye sahip Ã¶zellikleri belirledim

2. Veri HazÄ±rlama ğŸ§¼
- Veri setini eÄŸitim (train) ve test olmak Ã¼zere iki kÄ±sma ayÄ±rdÄ±m
- FarklÄ± birimlere sahip Ã¶zelliklerin Ã¶lÃ§eklendirilmesi iÃ§in "MinMaxScaler" normalizasyon yÃ¶ntemini kullandÄ±m

ğŸ† Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

SonuÃ§lar ğŸ“Š
- DoÄŸruluk (Accuracy): Her iki model de aynÄ± sonucu verdi âœ…
- HÄ±z PerformansÄ±: Sklearn modeli, custom modele gÃ¶re yaklaÅŸÄ±k **200 kat** daha hÄ±zlÄ± ğŸš€

Ã‡Ä±karÄ±mlar ğŸ’¡
- Kendi modelimin bu kadar yavaÅŸ olmasÄ± numpy fonksiyonlarÄ±nÄ± optimize kullanmamam ve sabit bir iterasyon sayÄ±sÄ±na sahip olmam olabilir. AynÄ± ÅŸekilde kullandÄ±ÄŸÄ±m gradyan decent algoritmasÄ±
  Ã§ok temeldi. Belki erken durdurma gibi teknikler, geliÅŸmiÅŸ bir gradient decent algoritmasÄ± veya AdamW optimizer gibi Ã¶ÄŸrenme hÄ±zÄ± optimize ediciler kullanmak kendi modelimin hÄ±zÄ±nÄ± artÄ±rabilir.

ğŸŒŸ Ã–ÄŸrenilen Dersler
- Makine Ã¶ÄŸrenmesi modellerinin kodlanmasÄ±
- Veri Ã¶n iÅŸleme teknikleri
- FarklÄ± yaklaÅŸÄ±mlarla aynÄ± problemi Ã§Ã¶zme becerisi

ğŸ“Œ Not
Bu proje, teorik bilgilerin pratiÄŸe dÃ¶kÃ¼lmesinde Ã¶nemli bir adÄ±m olmuÅŸtur. ğŸ“
